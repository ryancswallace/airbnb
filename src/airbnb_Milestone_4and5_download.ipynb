{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Airbnb Pricing Prediction: Milestone 4 & 5\n",
    "**James Gearheart**<br>\n",
    "**Danny Zhuang**<br>\n",
    "**Bob Saludo**<br>\n",
    "**Ryan Wallace**<br><br>\n",
    "**Harvard University**<br>\n",
    "**Fall 2016**<br>\n",
    "**TF: Christine Hwang**<br>\n",
    "**Due Date: ** Saturday, November 28th, 2016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of Work and Insights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Milestone 4\n",
    "\n",
    "For our baseline model, we were to fit a linear regression model using the relevant features to predict price. However, the main hurdle in this task was massaging/manipulating each of the raw features so that we could fit our model while achieving interpretability and computational efficiency. \n",
    "\n",
    "To incorporate the days of the week and holidays, we used our work on the “Average Difference from Listing’s Own Mean Price” from Milestone 3 in visualizing how prices changed throughout the year. Ultimately, we found that the real increase in per-night rental costs came on Friday, Saturday, and around the New Years’ holidays. Additionally, we found that non-holiday dates in January and February showed the lowest prices, which we deem as “slump” dates. Thus, we made categorical variables to denote the day of the week (weekend or no weekend), holiday (3 days around New Years’), and slump dates (January and February dates that aren’t around New Years’). \n",
    "\n",
    "Because the categorical variables of neighborhood and zipcode have over 200 distinct values each, one-hot encoding would produce to far too many variables for a linear regression model – leading to long computational time and a small chance of over-fitting.  To solve this, we create four categorical variables for each of these features that separate the neighborhoods and zipcodes by price into quartiles. Each quartile is its own categorical variables (e.g. most expensive 25% of neighborhoods, least expensive 25% of zipcodes). Thus, we move away from trying to account for individual neighborhoods such as “Tribeca” separately and instead choose to analyze the most expensive neighborhoods together.  While we lose some degree of granularity, we believe that what is gained in computational efficiency and streamlined interpretability is well worth it. \n",
    "\n",
    "### Milestone 5\n",
    "\n",
    "Although many of the data transformations were created and visualized in the Milestone 3 assignment, one of the key items that could net beneficial results is to create a clustering algorithm for zip codes and neighborhoods.  The current methodology calculates the median price for each of the zip codes and neighborhoods and sorts them by their median price.  This list of zip codes and their median prices are then grouped by quartiles.  Although this method provided a net lift in the predictive model’s accuracy, we believe that an algorithmic approach based on k-means clustering would be a better approach to determine related groups.   \n",
    "\n",
    "The current baseline model to predict daily price for the AirBnB dataset is a linear regression model which currently has an R^2 of 37%.  We believe that the predictive accuracy could be improved by exploring additional predictive models designed for a continuous dependant variable.  We will build and analyze Ridge Regression and Lasso Regression models to compare to our baseline standard linear regression model.  These models are designed to predict continuous data that has a large number of predictive variables.  We believe that one of these models will provide us with greater accuracy than the current baseline model.\n",
    "\n",
    "Since our baseline and proposed methods result in a continuous number price prediction (say, $109.01 per night), we ultimately hope to generate an interval around our continuous predictions in order to provide a range of pricing options, as this seems more reasonable and useful for AirBnB hosts. We plan to do this by analyzing the prediction or confidence intervals for the results of our regressions to create ranges around our predictions in which we hope a certain fixed percentage of the true prices fall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import csv\n",
    "import datetime\n",
    "import operator\n",
    "import random\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression as LinReg\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/RyanWallace/anaconda/lib/python2.7/site-packages/IPython/core/interactiveshell.py:2723: DtypeWarning: Columns (0,2,6,17) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "# read the three datasets\n",
    "listings_df = pd.read_csv('datasets/listings_final.csv', delimiter=',', index_col=0)\n",
    "reviews_df = pd.read_csv('datasets/reviews_final.csv', delimiter=',', index_col=0)\n",
    "calendar_df = pd.read_csv('datasets/calendar_final.csv', index_col=0)\n",
    "\n",
    "# log transform prices in calendar\n",
    "calendar_df['price_log'] = np.log(calendar_df['price'])\n",
    "\n",
    "# create calendar with listings data added\n",
    "calendar_expanded_df = calendar_df.merge(listings_df, on='listing_id', how='left', suffixes=['_calendar', '_listings'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# strip errant NaN's\n",
    "cols = ['bathrooms', 'bedrooms', 'beds', 'accommodates', 'longitude', 'neighbourhood', \n",
    "        'zipcode', 'date', 'price_log_calendar']\n",
    "calendar_expanded_df.replace([np.inf, -np.inf], np.nan)\n",
    "calendar_expanded_df = calendar_expanded_df.dropna(subset=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# method to convert date to day of week\n",
    "def get_day(date):\n",
    "    return datetime.datetime.strptime(date, '%Y-%m-%d').strftime('%A')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create indicators for time variables\n",
    "# weekend\n",
    "dates = np.array(calendar_expanded_df['date'])\n",
    "days = [get_day(date) for date in dates]\n",
    "weekend = [1 if day == 'Friday' or day == 'Saturday' else 0 for day in days]\n",
    "calendar_expanded_df['weekend'] = pd.Series(np.array(weekend), index=calendar_expanded_df.index)\n",
    "\n",
    "# major holidys (around New Years)\n",
    "holiday_dates = ['2015-01-01', '2015-01-02', '2015-01-03']\n",
    "holiday = [1 if date in holiday_dates else 0 for date in dates]\n",
    "calendar_expanded_df['holiday'] = pd.Series(np.array(holiday), index=calendar_expanded_df.index)\n",
    "\n",
    "# not January (excluding Holidays) or February\n",
    "slump_dates = []\n",
    "for d in range(4, 10):\n",
    "    slump_dates.append('2015-01-0' + str(d))\n",
    "for d in range(10, 32):\n",
    "    slump_dates.append('2015-01-' + str(d))\n",
    "for d in range(1, 10):\n",
    "    slump_dates.append('2015-02-0' + str(d))\n",
    "for d in range(10, 29):\n",
    "    slump_dates.append('2015-01-' + str(d))\n",
    "slump = [1 if date in slump_dates else 0 for date in dates]\n",
    "calendar_expanded_df['slump'] = pd.Series(np.array(slump), index=calendar_expanded_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# find means by zipcode and group into zipcodes\n",
    "neighborhoods = calendar_expanded_df['neighbourhood'].unique()\n",
    "zipcodes = calendar_expanded_df['zipcode'].unique()\n",
    "\n",
    "neighborhood_prices = []\n",
    "for neighborhood in neighborhoods:\n",
    "    neighborhood_prices.append((neighborhood, np.mean(np.array(listings_df[listings_df['neighbourhood'] == neighborhood]['price']))))\n",
    "\n",
    "zipcode_prices = []\n",
    "for zipcode in zipcodes:\n",
    "    zipcode_prices.append((zipcode, np.mean(np.array(listings_df[listings_df['zipcode'] == zipcode]['price']))))\n",
    "    \n",
    "# group zipcodes and neighborhoods into quartiles by average\n",
    "neighborhood_prices.sort(key=operator.itemgetter(1), reverse=True)\n",
    "zipcode_prices.sort(key=operator.itemgetter(1), reverse=True)\n",
    "\n",
    "# find size of quartiles\n",
    "neighborhood_quartile_size = int(np.round(len(neighborhood_prices)*0.25))\n",
    "zipcode_quartile_size = int(np.round(len(zipcode_prices)*0.25))\n",
    "\n",
    "# break up neighboorhood and zipcodes by quartile\n",
    "neighborhood_1 = neighborhood_prices[:neighborhood_quartile_size]\n",
    "neighborhood_2 = neighborhood_prices[neighborhood_quartile_size:2*neighborhood_quartile_size]\n",
    "neighborhood_3 = neighborhood_prices[2*neighborhood_quartile_size:3*neighborhood_quartile_size]\n",
    "neighborhood_4 = neighborhood_prices[3*neighborhood_quartile_size:]\n",
    "\n",
    "zipcode_1 = zipcode_prices[:zipcode_quartile_size]\n",
    "zipcode_2 = zipcode_prices[zipcode_quartile_size:2*zipcode_quartile_size]\n",
    "zipcode_3 = zipcode_prices[2*zipcode_quartile_size:3*zipcode_quartile_size]\n",
    "zipcode_4 = zipcode_prices[3*zipcode_quartile_size:]\n",
    "\n",
    "# create new indicators for each quartile\n",
    "neighborhoods = np.array(calendar_expanded_df['neighbourhood'])\n",
    "zipcodes = np.array(calendar_expanded_df['zipcode'])\n",
    "\n",
    "neighborhood_q1 = []\n",
    "neighborhood_q2 = []\n",
    "neighborhood_q3 = []\n",
    "neighborhood_q4 = []\n",
    "\n",
    "zipcode_q1 = []\n",
    "zipcode_q2 = []\n",
    "zipcode_q3 = []\n",
    "zipcode_q4 = []\n",
    "\n",
    "for neighborhood in neighborhoods:\n",
    "    if neighborhood in neighborhood_1:\n",
    "        neighborhood_q1.append(1)\n",
    "    else:\n",
    "        neighborhood_q1.append(0)\n",
    "    \n",
    "    if neighborhood in neighborhood_2:\n",
    "        neighborhood_q2.append(1)\n",
    "    else:\n",
    "        neighborhood_q2.append(0)\n",
    "    \n",
    "    if neighborhood in neighborhood_3:\n",
    "        neighborhood_q3.append(1)\n",
    "    else:\n",
    "        neighborhood_q3.append(0)\n",
    "        \n",
    "    if neighborhood in neighborhood_4:\n",
    "        neighborhood_q4.append(1)\n",
    "    else:\n",
    "        neighborhood_q4.append(0)\n",
    "        \n",
    "for zipcode in zipcodes:\n",
    "    if zipcode in zipcode_1:\n",
    "        zipcode_q1.append(1)\n",
    "    else:\n",
    "        zipcode_q1.append(0)\n",
    "    \n",
    "    if zipcode in zipcode_2:\n",
    "        zipcode_q2.append(1)\n",
    "    else:\n",
    "        zipcode_q2.append(0)\n",
    "    \n",
    "    if zipcode in zipcode_3:\n",
    "        zipcode_q3.append(1)\n",
    "    else:\n",
    "        zipcode_q3.append(0)\n",
    "        \n",
    "    if zipcode in zipcode_4:\n",
    "        zipcode_q4.append(1)\n",
    "    else:\n",
    "        zipcode_q4.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# extract relevant feature listing\n",
    "relevant_vars = ['bathrooms', 'bedrooms', 'beds', 'accommodates', 'longitude', \n",
    "                 'weekend', 'holiday', 'slump']\n",
    "X_df = calendar_expanded_df[relevant_vars].copy()\n",
    "y_df = calendar_expanded_df[['price_log_calendar']].copy()\n",
    "\n",
    "# numpy for sklearn\n",
    "X = X_df.as_matrix()\n",
    "y = y_df.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  1.        ,  2.        , ...,  0.        ,\n",
       "         0.        ,  6.39692974],\n",
       "       [ 1.        ,  1.        ,  2.        , ...,  0.        ,\n",
       "         0.        ,  6.39692974],\n",
       "       [ 1.        ,  1.        ,  2.        , ...,  0.        ,\n",
       "         0.        ,  6.39692974],\n",
       "       ..., \n",
       "       [ 1.        ,  1.        ,  1.        , ...,  0.        ,\n",
       "         0.        ,  4.60517025],\n",
       "       [ 1.        ,  1.        ,  1.        , ...,  0.        ,\n",
       "         0.        ,  4.60517025],\n",
       "       [ 1.        ,  1.        ,  1.        , ...,  0.        ,\n",
       "         0.        ,  4.60517025]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert zipcode, neighborhood lists to np arrays\n",
    "zipcode_q1 = np.resize(np.array(zipcode_q1), (len(zipcode_q1), 1))\n",
    "zipcode_q2 = np.resize(np.array(zipcode_q2), (len(zipcode_q2), 1))\n",
    "zipcode_q3 = np.resize(np.array(zipcode_q3), (len(zipcode_q3), 1))\n",
    "zipcode_q4 = np.resize(np.array(zipcode_q4), (len(zipcode_q4), 1))\n",
    "\n",
    "neighborhood_q1 = np.resize(np.array(neighborhood_q1), (len(neighborhood_q1), 1))\n",
    "neighborhood_q2 = np.resize(np.array(neighborhood_q2), (len(neighborhood_q2), 1))\n",
    "neighborhood_q3 = np.resize(np.array(neighborhood_q3), (len(neighborhood_q3), 1))\n",
    "neighborhood_q4 = np.resize(np.array(neighborhood_q4), (len(neighborhood_q4), 1))\n",
    "\n",
    "# add categorical vars to X\n",
    "Xy = np.concatenate((X, zipcode_q1, zipcode_q2, zipcode_q3, zipcode_q4, \n",
    "                neighborhood_q1, neighborhood_q2, neighborhood_q3, \n",
    "                neighborhood_q4, y), axis=1)\n",
    "\n",
    "# format for sklearn\n",
    "Xy.astype(np.float32, copy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split into training and testing\n",
    "# use 75% for training, the rest for testing\n",
    "num_train = int(np.round(Xy.shape[0]*0.75))\n",
    "\n",
    "# shuffle for random selection\n",
    "random.shuffle(Xy)\n",
    "\n",
    "# pull out sets\n",
    "X_train = Xy[:num_train,:(-1)]\n",
    "X_test = Xy[num_train:,:(-1)]\n",
    "y_train = Xy[:num_train, -1]\n",
    "y_test = Xy[num_train:, -1]\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "y_train = np.nan_to_num(y_train)\n",
    "y_test = np.nan_to_num(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "array must not contain infs or NaNs",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-47d8d0ed1002>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# fit simple linear regression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mlinear_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLinReg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mlinear_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m'R^2 in test: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinear_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/RyanWallace/anaconda/lib/python2.7/site-packages/sklearn/linear_model/base.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    452\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_residues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrank_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msingular_\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m                 \u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstsq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/RyanWallace/anaconda/lib/python2.7/site-packages/scipy/linalg/basic.pyc\u001b[0m in \u001b[0;36mlstsq\u001b[0;34m(a, b, cond, overwrite_a, overwrite_b, check_finite, lapack_driver)\u001b[0m\n\u001b[1;32m    820\u001b[0m     \"\"\"\n\u001b[1;32m    821\u001b[0m     \u001b[0ma1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_asarray_validated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_finite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_finite\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 822\u001b[0;31m     \u001b[0mb1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_asarray_validated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_finite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_finite\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    823\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'expected matrix'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/RyanWallace/anaconda/lib/python2.7/site-packages/scipy/_lib/_util.pyc\u001b[0m in \u001b[0;36m_asarray_validated\u001b[0;34m(a, check_finite, sparse_ok, objects_ok, mask_ok, as_inexact)\u001b[0m\n\u001b[1;32m    185\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'masked arrays are not supported'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0mtoarray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray_chkfinite\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcheck_finite\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mobjects_ok\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'O'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/RyanWallace/anaconda/lib/python2.7/site-packages/numpy/lib/function_base.pyc\u001b[0m in \u001b[0;36masarray_chkfinite\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m   1031\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchar\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtypecodes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'AllFloat'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfinite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1032\u001b[0m         raise ValueError(\n\u001b[0;32m-> 1033\u001b[0;31m             \"array must not contain infs or NaNs\")\n\u001b[0m\u001b[1;32m   1034\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1035\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: array must not contain infs or NaNs"
     ]
    }
   ],
   "source": [
    "# fit simple linear regression\n",
    "linear_model = LinReg()\n",
    "linear_model.fit(X_train, y_train)\n",
    "print 'R^2 in test: ', linear_model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
